{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, auc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "# warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split(samples, labels, test_size):\n",
    "#     return train_test_split(samples, labels,stratify=labels, test_size=test_size,random_state=45)\n",
    "    return train_test_split(samples, labels,test_size=test_size,random_state=29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import pandas as pd \n",
    "\n",
    "data = joblib.load('tsne_1.sav')\n",
    "\n",
    "data = np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = []\n",
    "for i in range(len(data[:,0].tolist())):\n",
    "    temp.append(data[:,0].tolist()[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=temp\n",
    "Y=data[:,1]\n",
    "\n",
    "test_size=0.2\n",
    "X_train, X_test, y_train, y_test = get_split(X,Y.tolist(),test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(100,50), random_state=1,activation='relu')\n",
    "\n",
    "ml=MLPClassifier(hidden_layer_sizes=(256,256,10), max_iter=200, alpha=1e-4,\n",
    "                    solver='adam', verbose=10,\n",
    "                    learning_rate_init=0.001,activation='relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.31434005\n",
      "Iteration 2, loss = 2.30213902\n",
      "Iteration 3, loss = 2.29913447\n",
      "Iteration 4, loss = 2.29682184\n",
      "Iteration 5, loss = 2.29449283\n",
      "Iteration 6, loss = 2.29160314\n",
      "Iteration 7, loss = 2.28845529\n",
      "Iteration 8, loss = 2.28440417\n",
      "Iteration 9, loss = 2.28027801\n",
      "Iteration 10, loss = 2.27510954\n",
      "Iteration 11, loss = 2.26892911\n",
      "Iteration 12, loss = 2.26217077\n",
      "Iteration 13, loss = 2.25517278\n",
      "Iteration 14, loss = 2.24586701\n",
      "Iteration 15, loss = 2.23776725\n",
      "Iteration 16, loss = 2.22814776\n",
      "Iteration 17, loss = 2.21781545\n",
      "Iteration 18, loss = 2.20766734\n",
      "Iteration 19, loss = 2.19677974\n",
      "Iteration 20, loss = 2.18522148\n",
      "Iteration 21, loss = 2.17330974\n",
      "Iteration 22, loss = 2.16053829\n",
      "Iteration 23, loss = 2.14946083\n",
      "Iteration 24, loss = 2.13742240\n",
      "Iteration 25, loss = 2.12337124\n",
      "Iteration 26, loss = 2.11126529\n",
      "Iteration 27, loss = 2.10058145\n",
      "Iteration 28, loss = 2.08762090\n",
      "Iteration 29, loss = 2.07433930\n",
      "Iteration 30, loss = 2.06319744\n",
      "Iteration 31, loss = 2.05163518\n",
      "Iteration 32, loss = 2.03801586\n",
      "Iteration 33, loss = 2.02734050\n",
      "Iteration 34, loss = 2.01749031\n",
      "Iteration 35, loss = 2.00563116\n",
      "Iteration 36, loss = 1.99559754\n",
      "Iteration 37, loss = 1.98529417\n",
      "Iteration 38, loss = 1.97287530\n",
      "Iteration 39, loss = 1.96365561\n",
      "Iteration 40, loss = 1.95112267\n",
      "Iteration 41, loss = 1.94206142\n",
      "Iteration 42, loss = 1.93242923\n",
      "Iteration 43, loss = 1.92107809\n",
      "Iteration 44, loss = 1.91152051\n",
      "Iteration 45, loss = 1.90190102\n",
      "Iteration 46, loss = 1.89134374\n",
      "Iteration 47, loss = 1.88277855\n",
      "Iteration 48, loss = 1.87141222\n",
      "Iteration 49, loss = 1.86146493\n",
      "Iteration 50, loss = 1.85435776\n",
      "Iteration 51, loss = 1.84509677\n",
      "Iteration 52, loss = 1.83953480\n",
      "Iteration 53, loss = 1.82762377\n",
      "Iteration 54, loss = 1.81939910\n",
      "Iteration 55, loss = 1.81293442\n",
      "Iteration 56, loss = 1.80534006\n",
      "Iteration 57, loss = 1.79409003\n",
      "Iteration 58, loss = 1.78373132\n",
      "Iteration 59, loss = 1.77839546\n",
      "Iteration 60, loss = 1.77276870\n",
      "Iteration 61, loss = 1.76447888\n",
      "Iteration 62, loss = 1.75761966\n",
      "Iteration 63, loss = 1.74472671\n",
      "Iteration 64, loss = 1.74112077\n",
      "Iteration 65, loss = 1.73331303\n",
      "Iteration 66, loss = 1.72738184\n",
      "Iteration 67, loss = 1.72032796\n",
      "Iteration 68, loss = 1.71301534\n",
      "Iteration 69, loss = 1.70679717\n",
      "Iteration 70, loss = 1.69620824\n",
      "Iteration 71, loss = 1.68791293\n",
      "Iteration 72, loss = 1.68512473\n",
      "Iteration 73, loss = 1.67749968\n",
      "Iteration 74, loss = 1.66966985\n",
      "Iteration 75, loss = 1.66666856\n",
      "Iteration 76, loss = 1.65785917\n",
      "Iteration 77, loss = 1.64895363\n",
      "Iteration 78, loss = 1.64432154\n",
      "Iteration 79, loss = 1.64173840\n",
      "Iteration 80, loss = 1.63340210\n",
      "Iteration 81, loss = 1.62735632\n",
      "Iteration 82, loss = 1.61906891\n",
      "Iteration 83, loss = 1.62019589\n",
      "Iteration 84, loss = 1.61534143\n",
      "Iteration 85, loss = 1.60620983\n",
      "Iteration 86, loss = 1.60230802\n",
      "Iteration 87, loss = 1.59003160\n",
      "Iteration 88, loss = 1.59129041\n",
      "Iteration 89, loss = 1.58855478\n",
      "Iteration 90, loss = 1.57555431\n",
      "Iteration 91, loss = 1.57599804\n",
      "Iteration 92, loss = 1.57176344\n",
      "Iteration 93, loss = 1.56068426\n",
      "Iteration 94, loss = 1.55619516\n",
      "Iteration 95, loss = 1.55495855\n",
      "Iteration 96, loss = 1.54782277\n",
      "Iteration 97, loss = 1.54110530\n",
      "Iteration 98, loss = 1.53673075\n",
      "Iteration 99, loss = 1.53616722\n",
      "Iteration 100, loss = 1.52643232\n",
      "Iteration 101, loss = 1.52148623\n",
      "Iteration 102, loss = 1.51934937\n",
      "Iteration 103, loss = 1.51667932\n",
      "Iteration 104, loss = 1.50975816\n",
      "Iteration 105, loss = 1.50760718\n",
      "Iteration 106, loss = 1.49823797\n",
      "Iteration 107, loss = 1.49528227\n",
      "Iteration 108, loss = 1.49191471\n",
      "Iteration 109, loss = 1.48766927\n",
      "Iteration 110, loss = 1.48350262\n",
      "Iteration 111, loss = 1.48087811\n",
      "Iteration 112, loss = 1.47350831\n",
      "Iteration 113, loss = 1.47618791\n",
      "Iteration 114, loss = 1.46606767\n",
      "Iteration 115, loss = 1.46073173\n",
      "Iteration 116, loss = 1.45978576\n",
      "Iteration 117, loss = 1.45364068\n",
      "Iteration 118, loss = 1.45160862\n",
      "Iteration 119, loss = 1.44967241\n",
      "Iteration 120, loss = 1.44452945\n",
      "Iteration 121, loss = 1.44442118\n",
      "Iteration 122, loss = 1.43845157\n",
      "Iteration 123, loss = 1.43141456\n",
      "Iteration 124, loss = 1.42358017\n",
      "Iteration 125, loss = 1.42368253\n",
      "Iteration 126, loss = 1.42396316\n",
      "Iteration 127, loss = 1.42084940\n",
      "Iteration 128, loss = 1.41210072\n",
      "Iteration 129, loss = 1.40860208\n",
      "Iteration 130, loss = 1.40722796\n",
      "Iteration 131, loss = 1.40134529\n",
      "Iteration 132, loss = 1.40217921\n",
      "Iteration 133, loss = 1.39513094\n",
      "Iteration 134, loss = 1.39339942\n",
      "Iteration 135, loss = 1.39132112\n",
      "Iteration 136, loss = 1.38440879\n",
      "Iteration 137, loss = 1.38498889\n",
      "Iteration 138, loss = 1.38327927\n",
      "Iteration 139, loss = 1.37480465\n",
      "Iteration 140, loss = 1.37377845\n",
      "Iteration 141, loss = 1.36837966\n",
      "Iteration 142, loss = 1.36441645\n",
      "Iteration 143, loss = 1.36651023\n",
      "Iteration 144, loss = 1.36538541\n",
      "Iteration 145, loss = 1.35176682\n",
      "Iteration 146, loss = 1.35436941\n",
      "Iteration 147, loss = 1.35038175\n",
      "Iteration 148, loss = 1.34883164\n",
      "Iteration 149, loss = 1.34479578\n",
      "Iteration 150, loss = 1.34300898\n",
      "Iteration 151, loss = 1.34258557\n",
      "Iteration 152, loss = 1.33748293\n",
      "Iteration 153, loss = 1.32896954\n",
      "Iteration 154, loss = 1.33204438\n",
      "Iteration 155, loss = 1.33150006\n",
      "Iteration 156, loss = 1.32300103\n",
      "Iteration 157, loss = 1.32056715\n",
      "Iteration 158, loss = 1.32125468\n",
      "Iteration 159, loss = 1.31922147\n",
      "Iteration 160, loss = 1.31384449\n",
      "Iteration 161, loss = 1.31501111\n",
      "Iteration 162, loss = 1.31652157\n",
      "Iteration 163, loss = 1.30546952\n",
      "Iteration 164, loss = 1.30519448\n",
      "Iteration 165, loss = 1.30171156\n",
      "Iteration 166, loss = 1.29654522\n",
      "Iteration 167, loss = 1.29760211\n",
      "Iteration 168, loss = 1.29083585\n",
      "Iteration 169, loss = 1.28714936\n",
      "Iteration 170, loss = 1.28794903\n",
      "Iteration 171, loss = 1.28941993\n",
      "Iteration 172, loss = 1.28095644\n",
      "Iteration 173, loss = 1.27834153\n",
      "Iteration 174, loss = 1.27203395\n",
      "Iteration 175, loss = 1.27197550\n",
      "Iteration 176, loss = 1.27188494\n",
      "Iteration 177, loss = 1.27280680\n",
      "Iteration 178, loss = 1.27263557\n",
      "Iteration 179, loss = 1.26522322\n",
      "Iteration 180, loss = 1.26753892\n",
      "Iteration 181, loss = 1.25809506\n",
      "Iteration 182, loss = 1.25858073\n",
      "Iteration 183, loss = 1.26077980\n",
      "Iteration 184, loss = 1.25732991\n",
      "Iteration 185, loss = 1.24746616\n",
      "Iteration 186, loss = 1.25207075\n",
      "Iteration 187, loss = 1.25368062\n",
      "Iteration 188, loss = 1.24984082\n",
      "Iteration 189, loss = 1.23659326\n",
      "Iteration 190, loss = 1.23877141\n",
      "Iteration 191, loss = 1.24580158\n",
      "Iteration 192, loss = 1.23476860\n",
      "Iteration 193, loss = 1.22955195\n",
      "Iteration 194, loss = 1.22955763\n",
      "Iteration 195, loss = 1.22533527\n",
      "Iteration 196, loss = 1.22754151\n",
      "Iteration 197, loss = 1.21973447\n",
      "Iteration 198, loss = 1.22426705\n",
      "Iteration 199, loss = 1.21758719\n",
      "Iteration 200, loss = 1.21543459\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(256, 256, 10), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=10, warm_start=False)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09913289978322494"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = ml.predict(X_test)\n",
    "\n",
    "accuracy_score(y_test,y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5815858590069621"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = ml.predict(X_train)\n",
    "\n",
    "accuracy_score(y_train,y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, z_dim, num_classes):\n",
    "        super(Classifier, self).__init__()\n",
    "\n",
    "        self.fc_model = nn.Sequential(OrderedDict([\n",
    "            ('fc_1', nn.Linear(in_features=z_dim, out_features=256, bias=True)),\n",
    "            ('fc_1_bn', nn.BatchNorm1d(num_features=256)),\n",
    "            ('LeakyRelu_1', nn.LeakyReLU(negative_slope=0.2, inplace=True)),\n",
    "\n",
    "            ('fc_2', nn.Linear(in_features=256, out_features=256, bias=True)),\n",
    "            ('fc_2_bn', nn.BatchNorm1d(num_features=256)),\n",
    "            ('LeakyRelu_2', nn.LeakyReLU(negative_slope=0.2, inplace=True)),\n",
    "\n",
    "            ('fc_3', nn.Linear(in_features=256, out_features=num_classes, bias=True))\n",
    "        ]))\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = self.fc_model(z)\n",
    "\n",
    "        return x\n",
    "\n",
    "    \n",
    "classifier = Classifier(z_dim=16, num_classes=10)\n",
    "classifier_pred = classifier([X_train,y_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
